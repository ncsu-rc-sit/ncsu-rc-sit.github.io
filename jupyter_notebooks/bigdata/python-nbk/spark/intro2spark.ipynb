{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](image1.PNG)\n",
    "\n",
    "## spark\n",
    "\n",
    "Apache Spark is an open-source cluster-computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. Spark it is a fast and general engine for large-scale data processing.\n",
    "Databricks is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark.\n",
    "\n",
    "## Spark context\n",
    "\n",
    "In Spark, communication occurs between a driver and executors. The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion. Executor programs run on cluster nodes or in local threads. The results from these tasks are delivered back to the driver.\n",
    "The code runs\n",
    "+ Locally, in the driver\n",
    "+ Distributed at the executors (Executors run in parallel and have much more memory)\n",
    "+ Both at the driver and the executors\n",
    "\n",
    "Problems with cheap hardware are:\n",
    "+ failures.\n",
    "+ network speeds versus shared memory.\n",
    "+ much more latency.\n",
    "+ network slower than storage.\n",
    "+ uneven performance.\n",
    "\n",
    "## Spark Capabilities\n",
    "\n",
    "+ Performance\n",
    "   + First, use RAM\n",
    "   + Also, be smarter\n",
    "   \n",
    "+ Ease of Use\n",
    "   + Python, Scala, Java first class citizens\n",
    "   \n",
    "+ New Paradigms\n",
    "   + SparkSQL\n",
    "   + Streaming\n",
    "   + MLib\n",
    "   + GraphX\n",
    "   + …more\n",
    "\n",
    "![image2](image2.PNG)\n",
    "\n",
    "## Spark Formula\n",
    "1. Create/Load RDD\n",
    "\n",
    "2. Transform RDD\n",
    "\n",
    "3. But don’t do anything yet!\n",
    "\n",
    "4. Perform Actions that return data\n",
    "\n",
    "Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124787"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "lines_rdd = sc.textFile (\"Complete_Shakespeare.txt\")\n",
    "lines_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "The first thing a Spark program requires is a context, which interfaces with some kind of cluster to use. Our\n",
    "pyspark shell provides us with a convenient `sc`, using the local filesystem, to start. Your standalone programs\n",
    "will have to specify one:\n",
    "\n",
    "```PYTHON\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Test_App\")\n",
    "sc = SparkContext(conf = conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_rdd = sc.textFile (\"Complete_Shakespeare.txt\")\n",
    "QueenLines_rdd = lines_rdd.filter(lambda line: \"queen\" in line)\n",
    "QueenLines_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  As on the finger of a throned queen,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueenLines_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambdas\n",
    "A lambda is simply a function that is too simple to deserve its own subroutine.\n",
    "Anywhere we have a lambda we could also just name a real subroutine that could go off and do anything.\n",
    "\n",
    "When all you want to do is see if “given an input variable line, is “stanford” in there?”, it isn’t worth the\n",
    "digression.\n",
    "\n",
    "Most modern languages have adopted this nicety.\n",
    "\n",
    "## Common Transformations\n",
    "\n",
    "![image3](image3.PNG)\n",
    "\n",
    "Click [here](\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\") for full list.\n",
    "\n",
    "# Common Actions\n",
    "\n",
    "![image4](image4.PNG)\n",
    "\n",
    "Clich [here]( http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) for full list.\n",
    "\n",
    "## Pair RDDs\n",
    "\n",
    "+ Key/Value organization is a simple, but often very efficient schema, as we mentioned in our NoSQL discussion.\n",
    "+  Spark provides special operations on RDDs that contain key/value pairs. They are similar to the general ones that we have seen.\n",
    "+  On the language (Python, Scala, Java) side key/values are simply tuples. If you have an RDD all of whose elements happen to be tuples of two items, it is a Pair RDD and you can use the key/value operations that follow.\n",
    "\n",
    "## Pair RDD Transformations\n",
    "\n",
    "![image5](image5.PNG)\n",
    "\n",
    "<mark> Note </mark>: All the regular transformations are also available.\n",
    "\n",
    "## Pair RDD Actions\n",
    "\n",
    "As with transformations, all of the regular actions are available to Pair RDDs, and there\n",
    "are some additional ones that can take advantage of key/value structure.\n",
    "\n",
    "![image6](image6.PNG)\n",
    "\n",
    "Click [here]( http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) for full list.\n",
    "\n",
    "## Two Pair RDD Transformations\n",
    "\n",
    "![image7](image7.PNG)\n",
    "\n",
    "Click [here]( http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) for full list.\n",
    "\n",
    "## Joins\n",
    "\n",
    "Any database designer can tell you how common joins are. Let's look at a simple\n",
    "example. We have (here we create it) an RDD of our top purchasing customers.\n",
    "\n",
    "And an RDD with all of our customers' addresses.\n",
    "\n",
    "To create a mailing list of special coupons for those favored customers we can use a\n",
    "join on the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_customers_rdd = sc.parallelize([(\"Joe\", \"$103\"), (\"Alice\", \"$2000\"), (\"Bob\", \"$1200\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_addresses_rdd = sc.parallelize([(\"Joe\", \"23 State St.\"), (\"Frank\", \"555 Timer Lane\"), (\"Sally\", \"44Forest Rd.\"), (\"Alice\", \"3 Elm Road\"), (\"Bob\", \"88 West Oak\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " promotion_mail_rdd = best_customers_rdd.join(customer_addresses_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob', ('$1200', '88 West Oak')),\n",
       " ('Joe', ('$103', '23 State St.')),\n",
       " ('Alice', ('$2000', '3 Elm Road'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promotion_mail_rdd.collect()\n",
    "[('Bob', ('$1200', '88 West Oak')), ('Joe', ('$103', '23 State St.')), ('Alice', ('$2000', '3 Elm Road'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare, data analysis\n",
    "\n",
    "1. Count the number of lines\n",
    "2. Count the number of words (hint: Python \"split\" is a workhorse)\n",
    "3. Count unique words\n",
    "4. Count the occurrence of each word\n",
    "5. Show the top 5 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_rdd = sc.textFile(\"Complete_Shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124787"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904061"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd = lines_rdd.flatMap(lambda x: x.split())\n",
    "words_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67779"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd = words_rdd.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1), ('Project', 1), ('Gutenberg', 1), ('EBook', 1), ('of', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3977),\n",
       " ('Project', 85),\n",
       " ('Gutenberg', 26),\n",
       " ('EBook', 2),\n",
       " ('of', 15649)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_rdd = key_value_rdd.reduceByKey(lambda x,y: x+y)\n",
    "word_counts_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3977, 'The'),\n",
       " (85, 'Project'),\n",
       " (26, 'Gutenberg'),\n",
       " (2, 'EBook'),\n",
       " (15649, 'of')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_rdd = word_counts_rdd.map(lambda x: (x[1],x[0]))\n",
    "flipped_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23407, 'the'), (19540, 'I'), (18358, 'and'), (15682, 'to'), (15649, 'of')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_rdd = flipped_rdd.sortByKey(False)\n",
    "results_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "\n",
    "We said one of the advantages of Spark is that we can control things for better\n",
    "performance. There are a multitude of optimization, performance, tuning and\n",
    "programmatic features to enable better control. We quickly look at a few of the most\n",
    "important.\n",
    "+ Persistence\n",
    "+ Partitioning\n",
    "+ Parallel Programming Capabilities\n",
    "+ Performance and Debugging Tools\n",
    "\n",
    "## Persistence\n",
    "\n",
    "+ Lazy evaluation implies by default that all the RDD dependencies will be computed when we call an action on that RDD.\n",
    "+ If we intend to use that data multiple times (say we are filtering some log, then dumping the results, but we will analyze it further) we can tell Spark to persist the data.\n",
    "+ We can specify different levels of persistence: `MEMORY_ONLY, MEMORY_ONLY_SER, MEMORY_AND_DISK, MEMORY_AND_DISK_SER, DISK_ONLY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_rdd = sc.textFile(\"Complete_Shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "QueenLines_rdd = lines_rdd.filter(lambda line: \"queen\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueenLines_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueenLines_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  As on the finger of a throned queen,'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueenLines_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "+ Spark distributes the data of your RDDs across its resources. It tries to do some obvious things.\n",
    "+ With key/value pairs we can help keep that data grouped efficiently.\n",
    "+ We can create custom partitioners that beat the default (which is probably a hash or maybe range).\n",
    "+ Use persist() if you have partitioned your data in some smart way. Otherwise it will keep getting re-partitioned.\n",
    "\n",
    "## Parallel Programming Features\n",
    "\n",
    "Spark has several parallel programming features that make it easier and more efficient to do operations in parallel in a more explicit way.\n",
    "\n",
    "Accumulators are variables that allow many copies of a variable to exist on the separate worker nodes.\n",
    "\n",
    "It is also possible to have replicated data that we would like all the workers to have access to. Perhaps a lookup table of IP addresses to\n",
    "country codes so that each worker can transform or filter on such information. Maybe we want to exclude all non-US IP entries in our logs.\n",
    "You might think of ways you could do this just by passing variables, but they would likely be expensive in actual operation (usually requiring\n",
    "multiple sends). The solution in Spark is to send an (immutable, read only) broadcast variable\n",
    "\n",
    "![image8](image8.PNG)\n",
    "\n",
    "## Performance & Debugging\n",
    "\n",
    "We will give unfortunately short shrift to performance and debugging, which are both\n",
    "important. Mostly, this is because they are very configuration and application\n",
    "dependent.\n",
    "Here are a few things to at least be aware of:\n",
    "+ SparkConf() class. A lot of options can be tweaked here.\n",
    "+ Spark Web UI. A very friendly way to explore all of these issues.\n",
    "\n",
    "## IO Formats\n",
    "\n",
    "Spark has an impressive, and growing, list of input/output formats it supports. Some important ones:\n",
    "\n",
    "+ Text\n",
    "+ CSV\n",
    "+ SQL type Query/Load\n",
    "  + JSON (can infer schema)\n",
    "  + Parquet\n",
    "  + Hive\n",
    "  + XML\n",
    "  + Sequence (Hadoopy key/value)\n",
    "  + Databases: JDBC, Cassandra, HBase, MongoDB, etc.\n",
    "+ Compression (gzip…)\n",
    "\n",
    "And it can interface directly with a variety of filesystems: local, HDFS, Lustre, Amazon S3,...\n",
    "\n",
    "# Creating DataFrames\n",
    "\n",
    "A row RDD is the basic way to go from RDD to DataFrame, and back, if necessary. A \"row\" is just a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_rdd = sc.parallelize([ (\"Joe\",\"Pine St.\",\"PA\",12543), (\"Sally\",\"Fir Dr.\",\"WA\",78456), (\"Jose\",\"Elm Pl.\",\"ND\",45698) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/7c/81b89b63927d7b24be7cbd61eabf90a28458bac90ff12531edd56d76ad61/spark-0.2.1.tar.gz (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 6.9MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: spark\n",
      "  Running setup.py bdist_wheel for spark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/57/a3/24/7c4c81b1cc5e82ad386965d2808543c4f5306b759da26e5629\n",
      "Successfully built spark\n",
      "Installing collected packages: spark\n",
      "Successfully installed spark-0.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "aDataFrameFromRDD = spark.createDataFrame( row_rdd, [\"name\", \"street\", \"state\", \"zip\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+-----+\n",
      "| name|  street|state|  zip|\n",
      "+-----+--------+-----+-----+\n",
      "|  Joe|Pine St.|   PA|12543|\n",
      "|Sally| Fir Dr.|   WA|78456|\n",
      "| Jose| Elm Pl.|   ND|45698|\n",
      "+-----+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aDataFrameFromRDD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
